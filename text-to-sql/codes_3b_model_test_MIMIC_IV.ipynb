{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Update the `PROJECT_PATH` variable to the absolute path of your `ehrsql-2024` project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n",
      "Schema Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/tables.json\n",
      "Questions Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/test/data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_PATH = 'C:/Uni/Bachelorarbeit/ehrsql-2024'\n",
    "\n",
    "DB_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/mimic_iv.sqlite')\n",
    "SCHEMA_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/tables.json')\n",
    "QUESTIONS_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/test/data.json')\n",
    "\n",
    "\n",
    "print(f\"Database Path: {DB_PATH}\")\n",
    "print(f\"Schema Path: {SCHEMA_PATH}\")\n",
    "print(f\"Questions Path: {QUESTIONS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"‚ùå ERROR: Database file not found. Please check your PROJECT_PATH.\")\n",
    "if not os.path.exists(SCHEMA_PATH):\n",
    "    print(\"‚ùå ERROR: Schema file not found. Please check your PROJECT_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful to: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n"
     ]
    }
   ],
   "source": [
    "def connect_to_db(db_path):\n",
    "    \"\"\"Create a connection engine for the SQLite database.\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'sqlite:///{db_path}')\n",
    "        with engine.connect() as conn:\n",
    "            print(f\"‚úÖ Database connection successful to: {db_path}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "db_engine = connect_to_db(DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Format the Database Schema\n",
    "\n",
    "This is a key step for Retrieval-Augmented Generation (RAG). We load the schema from `tables.json` and format it as a string to provide context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema context created successfully (with keys).\n"
     ]
    }
   ],
   "source": [
    "def get_schema_context(schema_path):\n",
    "    \"\"\"\n",
    "    Loads the database schema from tables.json and formats it for the LLM prompt,\n",
    "    including table names, column names, primary keys, and foreign keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(schema_path, 'r') as f:\n",
    "            schema_data = json.load(f)[0]  # The data is inside a list\n",
    "        \n",
    "        table_names = schema_data['table_names_original']\n",
    "        column_data = schema_data['column_names_original']\n",
    "        primary_keys_indices = schema_data['primary_keys']\n",
    "        foreign_keys_pairs = schema_data['foreign_keys']\n",
    "\n",
    "        # Map column index to table and column name\n",
    "        columns_map = {i: (table_names[col[0]], col[1]) for i, col in enumerate(column_data) if col[0] != -1}\n",
    "        \n",
    "        # Map table name to its columns\n",
    "        table_columns_map = {}\n",
    "        for table_index, table_name in enumerate(table_names):\n",
    "            table_columns_map[table_name] = [col[1] for col in column_data if col[0] == table_index]\n",
    "\n",
    "        # Get primary key column names\n",
    "        pk_map = {}\n",
    "        for pk_index in primary_keys_indices:\n",
    "            if pk_index in columns_map:\n",
    "                table_name, col_name = columns_map[pk_index]\n",
    "                if table_name not in pk_map:\n",
    "                    pk_map[table_name] = []\n",
    "                pk_map[table_name].append(col_name)\n",
    "\n",
    "        # Get foreign key relationships\n",
    "        fk_list = []\n",
    "        for fk_col_index, pk_col_index in foreign_keys_pairs:\n",
    "            if fk_col_index in columns_map and pk_col_index in columns_map:\n",
    "                fk_table, fk_col = columns_map[fk_col_index]\n",
    "                pk_table, pk_col = columns_map[pk_col_index]\n",
    "                fk_list.append(f\"{fk_table}.{fk_col} can be joined with {pk_table}.{pk_col}\")\n",
    "\n",
    "        # Build the context string\n",
    "        context_parts = []\n",
    "        for table_name in table_names:\n",
    "            columns = table_columns_map.get(table_name, [])\n",
    "            pk_info = \"\"\n",
    "            if table_name in pk_map:\n",
    "                pk_info = f\" (Primary Keys: {', '.join(pk_map[table_name])})\"\n",
    "            context_parts.append(f\"Table {table_name}, columns = [{', '.join(columns)}]{pk_info}\")\n",
    "        \n",
    "        schema_context = \"\\n\".join(context_parts)\n",
    "        if fk_list:\n",
    "            schema_context += \"\\n\\n-- Foreign Key Relationships:\\n\" + \"\\n\".join(fk_list)\n",
    "            \n",
    "        print(\"‚úÖ Schema context created successfully (with keys).\")\n",
    "        return schema_context\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load or parse schema file: {e}\")\n",
    "        return None\n",
    "\n",
    "schema_context = get_schema_context(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the LLM (CodeS-3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading CodeS-3B model...\n",
      "üì• This may take a few minutes on the first run as it downloads the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.54s/it]\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Model loaded successfully on cuda!\n"
     ]
    }
   ],
   "source": [
    "def load_codes_model():\n",
    "    \"\"\"Load the CodeS-3B model for SQL generation.\"\"\"\n",
    "    print(\"ü§ñ Loading CodeS-3B model...\")\n",
    "    print(\"üì• This may take a few minutes on the first run as it downloads the model.\")\n",
    "    \n",
    "    model_name = \"seeklhy/codes-3b\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üî• Model loaded successfully on {device}!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model, tokenizer = load_codes_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Core Functions for Text-to-SQL\n",
    "\n",
    "These functions will handle SQL generation and safe execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, schema: str, llm_model, llm_tokenizer):\n",
    "    \"\"\"Generate SQL using the loaded LLM with improved prompting.\"\"\"\n",
    "    if not llm_model or not llm_tokenizer:\n",
    "        return \"‚ùå Model not loaded\"\n",
    "\n",
    "    prompt = f\"\"\"### Instructions:\n",
    "Your task is to convert a question into a SQL query, given a database schema.\n",
    "Adhere to these rules:\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question.\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT t1.col1, t2.col2 FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id`.\n",
    "- **Look for medication names in the `prescriptions` table.** Match them against the `drug` column.\n",
    "- **Pay attention to the relationships between tables** outlined in the schema to construct correct JOIN statements.\n",
    "\n",
    "### Example:\n",
    "Question: What are the diagnoses for the patient with subject_id 10000032?\n",
    "Database Schema:\n",
    "Table patients, columns = [subject_id, gender, anchor_age, anchor_year, anchor_year_group, dod] (Primary Keys: subject_id)\n",
    "Table diagnoses_icd, columns = [subject_id, hadm_id, seq_num, icd_code, icd_version]\n",
    "Table d_icd_diagnoses, columns = [icd_code, icd_version, long_title] (Primary Keys: icd_code, icd_version)\n",
    "-- Foreign Key Relationships:\n",
    "diagnoses_icd.subject_id can be joined with patients.subject_id\n",
    "diagnoses_icd.icd_code can be joined with d_icd_diagnoses.icd_code\n",
    "\n",
    "SQL Query:\n",
    "SELECT T3.long_title FROM patients AS T1 JOIN diagnoses_icd AS T2 ON T1.subject_id = T2.subject_id JOIN d_icd_diagnoses AS T3 ON T2.icd_code = T3.icd_code AND T2.icd_version = T3.icd_version WHERE T1.subject_id = 10000032\n",
    "\n",
    "### Input:\n",
    "Question: {question}\n",
    "\n",
    "### Database Schema:\n",
    "{schema}\n",
    "\n",
    "### SQL Query:\"\"\"\n",
    "    \n",
    "    inputs = llm_tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=4096 # Increased max length for the detailed prompt\n",
    "    ).to(llm_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024, # Keep a generous limit for the output\n",
    "            do_sample=False,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # --- Robust SQL Extraction ---\n",
    "    # Find the start of the SQL block\n",
    "    sql_start_tag = \"```sql\"\n",
    "    sql_start_index = generated_text.find(sql_start_tag)\n",
    "    \n",
    "    if sql_start_index != -1:\n",
    "        # Find the end of the SQL block\n",
    "        sql_end_tag = \"```\"\n",
    "        sql_end_index = generated_text.find(sql_end_tag, sql_start_index + len(sql_start_tag))\n",
    "        if sql_end_index != -1:\n",
    "            # Extract the text between the tags\n",
    "            sql_query = generated_text[sql_start_index + len(sql_start_tag):sql_end_index].strip()\n",
    "        else:\n",
    "            # If no end tag, take the rest of the string after the start tag\n",
    "            sql_query = generated_text[sql_start_index + len(sql_start_tag):].strip()\n",
    "    else:\n",
    "        # Fallback for older models: find \"### SQL Query:\"\n",
    "        fallback_tag = \"### SQL Query:\"\n",
    "        fallback_index = generated_text.rfind(fallback_tag) # Use rfind to get the last occurrence\n",
    "        if fallback_index != -1:\n",
    "            sql_query = generated_text[fallback_index + len(fallback_tag):].strip()\n",
    "        else:\n",
    "            # If no tags are found, return a clear error message\n",
    "            return \"ERROR: Could not find SQL query in the generated text.\"\n",
    "\n",
    "    # Final cleanup to remove any trailing characters like semicolons\n",
    "    if ';' in sql_query:\n",
    "        sql_query = sql_query.split(';')[0]\n",
    "        \n",
    "    return sql_query\n",
    "\n",
    "def execute_sql(engine, query):\n",
    "    \"\"\"Execute a SQL query and return the result as a DataFrame.\"\"\"\n",
    "    if not engine:\n",
    "        return pd.DataFrame(), \"No database connection.\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_query(text(query), conn)\n",
    "        return df, \"‚úÖ Success\"\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), f\"‚ùå Query execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run a Test Query\n",
    "\n",
    "Let's test the full pipeline with a sample question from the EHRSQL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Has the prescription of sodium chloride 0.9%, nicardipine iv, or ondansetron been given to patient 10039997 in 2100?\n",
      "\n",
      "ü§ñ Generated SQL:\n",
      "```sql\n",
      "SELECT T1.subject_id, T1.gender, T1.dob, T1.dod, T2.long_title AS diagnosis, T3.long_title AS procedure, T4.label AS drug, T5.label AS dose_unit, T6.label AS route FROM patients AS T1 JOIN admissions AS T2 ON T1.row_id = T2.row_id JOIN diagnoses_icd AS T3 ON T2.row_id = T3.row_id JOIN d_icd_diagnoses AS T4 ON T3.icd_code = T4.icd_code AND T3.icd_version = T4.icd_version JOIN procedures_icd AS T5 ON T2.row_id = T5.row_id JOIN d_icd_procedures AS T6 ON T5.icd_code = T6.icd_code AND T5.icd_version = T6.icd_version JOIN labevents AS T7 ON T2.row_id = T7.row_id JOIN d_labitems AS T8 ON T7.itemid = T8.itemid JOIN prescriptions AS T9 ON T2.row_id = T9.row_id JOIN d_items AS T10 ON T9.drug = T10.itemid WHERE T1.subject_id = 10039997 AND T2.admittime BETWEEN 2100 AND 2100 AND T8.label IN ('Sodium chloride', 'Nicardipine iv', 'Ondansetron')\n",
      "```\n",
      "\n",
      "üìä Execution Result: ‚ùå Query execution error: (sqlite3.OperationalError) no such column: T2.long_title\n",
      "[SQL: SELECT T1.subject_id, T1.gender, T1.dob, T1.dod, T2.long_title AS diagnosis, T3.long_title AS procedure, T4.label AS drug, T5.label AS dose_unit, T6.label AS route FROM patients AS T1 JOIN admissions AS T2 ON T1.row_id = T2.row_id JOIN diagnoses_icd AS T3 ON T2.row_id = T3.row_id JOIN d_icd_diagnoses AS T4 ON T3.icd_code = T4.icd_code AND T3.icd_version = T4.icd_version JOIN procedures_icd AS T5 ON T2.row_id = T5.row_id JOIN d_icd_procedures AS T6 ON T5.icd_code = T6.icd_code AND T5.icd_version = T6.icd_version JOIN labevents AS T7 ON T2.row_id = T7.row_id JOIN d_labitems AS T8 ON T7.itemid = T8.itemid JOIN prescriptions AS T9 ON T2.row_id = T9.row_id JOIN d_items AS T10 ON T9.drug = T10.itemid WHERE T1.subject_id = 10039997 AND T2.admittime BETWEEN 2100 AND 2100 AND T8.label IN ('Sodium chloride', 'Nicardipine iv', 'Ondansetron')]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "ü§ñ Generated SQL:\n",
      "```sql\n",
      "SELECT T1.subject_id, T1.gender, T1.dob, T1.dod, T2.long_title AS diagnosis, T3.long_title AS procedure, T4.label AS drug, T5.label AS dose_unit, T6.label AS route FROM patients AS T1 JOIN admissions AS T2 ON T1.row_id = T2.row_id JOIN diagnoses_icd AS T3 ON T2.row_id = T3.row_id JOIN d_icd_diagnoses AS T4 ON T3.icd_code = T4.icd_code AND T3.icd_version = T4.icd_version JOIN procedures_icd AS T5 ON T2.row_id = T5.row_id JOIN d_icd_procedures AS T6 ON T5.icd_code = T6.icd_code AND T5.icd_version = T6.icd_version JOIN labevents AS T7 ON T2.row_id = T7.row_id JOIN d_labitems AS T8 ON T7.itemid = T8.itemid JOIN prescriptions AS T9 ON T2.row_id = T9.row_id JOIN d_items AS T10 ON T9.drug = T10.itemid WHERE T1.subject_id = 10039997 AND T2.admittime BETWEEN 2100 AND 2100 AND T8.label IN ('Sodium chloride', 'Nicardipine iv', 'Ondansetron')\n",
      "```\n",
      "\n",
      "üìä Execution Result: ‚ùå Query execution error: (sqlite3.OperationalError) no such column: T2.long_title\n",
      "[SQL: SELECT T1.subject_id, T1.gender, T1.dob, T1.dod, T2.long_title AS diagnosis, T3.long_title AS procedure, T4.label AS drug, T5.label AS dose_unit, T6.label AS route FROM patients AS T1 JOIN admissions AS T2 ON T1.row_id = T2.row_id JOIN diagnoses_icd AS T3 ON T2.row_id = T3.row_id JOIN d_icd_diagnoses AS T4 ON T3.icd_code = T4.icd_code AND T3.icd_version = T4.icd_version JOIN procedures_icd AS T5 ON T2.row_id = T5.row_id JOIN d_icd_procedures AS T6 ON T5.icd_code = T6.icd_code AND T5.icd_version = T6.icd_version JOIN labevents AS T7 ON T2.row_id = T7.row_id JOIN d_labitems AS T8 ON T7.itemid = T8.itemid JOIN prescriptions AS T9 ON T2.row_id = T9.row_id JOIN d_items AS T10 ON T9.drug = T10.itemid WHERE T1.subject_id = 10039997 AND T2.admittime BETWEEN 2100 AND 2100 AND T8.label IN ('Sodium chloride', 'Nicardipine iv', 'Ondansetron')]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "def run_full_test(question, schema, engine, llm_model, llm_tokenizer):\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    generated_query = generate_sql(question, schema, llm_model, llm_tokenizer)\n",
    "    print(f\"\\nü§ñ Generated SQL:\\n```sql\\n{generated_query}\\n```\")\n",
    "    \n",
    "    # Execute SQL\n",
    "    result_df, message = execute_sql(engine, generated_query)\n",
    "    \n",
    "    print(f\"\\nüìä Execution Result: {message}\")\n",
    "    if not result_df.empty:\n",
    "        display(result_df)\n",
    "\n",
    "# Load one question from the test set for testing\n",
    "try:\n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions_data = json.load(f)['data']\n",
    "    \n",
    "    test_question = questions_data[0]['question']\n",
    "    \n",
    "    # Run the test if all components are ready\n",
    "    if db_engine and schema_context and model and tokenizer:\n",
    "        run_full_test(test_question, schema_context, db_engine, model, tokenizer)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cannot run test. One or more components (DB, Schema, Model) failed to load.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load or run test questions: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
