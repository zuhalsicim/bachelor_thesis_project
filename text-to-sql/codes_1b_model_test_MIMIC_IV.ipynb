{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL with MIMIC-IV Demo & CodeS-1B Model\n",
    "\n",
    "This notebook provides a complete pipeline for converting natural language questions into SQL queries using a local LLM. It is configured to work with the **MIMIC-IV Demo database** (`mimic_iv_demo.sqlite`) and the resources from the **EHRSQL 2024 Shared Task**.\n",
    "\n",
    "### Workflow:\n",
    "1.  **Setup & Configuration**: Define paths to your database and project files.\n",
    "2.  **Load Database & Schema**: Connect to the SQLite database and load the schema context from `tables.json`.\n",
    "3.  **Load LLM**: Load the `seeklhy/codes-1b` model for SQL generation.\n",
    "4.  **Generate & Execute**: Take a natural language question, generate an SQL query, and execute it against the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.48.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "     ---------------------------------------- 0.0/374.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 374.7/374.7 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.43-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------------------------ --------------- 1.3/2.1 MB 41.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 34.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nuba\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (6.1.1)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.4-cp310-cp310-win_amd64.whl (298 kB)\n",
      "     ---------------------------------------- 0.0/298.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 298.7/298.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nuba\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nuba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Installing collected packages: greenlet, sqlalchemy, accelerate\n",
      "Successfully installed accelerate-1.10.0 greenlet-3.2.4 sqlalchemy-2.0.43\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers accelerate sqlalchemy pandas python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Update the `PROJECT_PATH` variable to the absolute path of your `ehrsql-2024` project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n",
      "Schema Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/tables.json\n",
      "Questions Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/test/data.json\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Set this to the path of your ehrsql-2024 folder\n",
    "# Example for Windows with WSL: '/mnt/c/Uni/Bachelorarbeit/ehrsql-2024/'\n",
    "# Example for standard Windows: 'C:/Uni/Bachelorarbeit/ehrsql-2024/'\n",
    "PROJECT_PATH = 'C:/Uni/Bachelorarbeit/ehrsql-2024'\n",
    "\n",
    "DB_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/mimic_iv.sqlite')\n",
    "SCHEMA_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/tables.json')\n",
    "QUESTIONS_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/test/data.json')\n",
    "\n",
    "\n",
    "print(f\"Database Path: {DB_PATH}\")\n",
    "print(f\"Schema Path: {SCHEMA_PATH}\")\n",
    "print(f\"Questions Path: {QUESTIONS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"❌ ERROR: Database file not found. Please check your PROJECT_PATH.\")\n",
    "if not os.path.exists(SCHEMA_PATH):\n",
    "    print(\"❌ ERROR: Schema file not found. Please check your PROJECT_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection successful to: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n"
     ]
    }
   ],
   "source": [
    "def connect_to_db(db_path):\n",
    "    \"\"\"Create a connection engine for the SQLite database.\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'sqlite:///{db_path}')\n",
    "        with engine.connect() as conn:\n",
    "            print(f\"✅ Database connection successful to: {db_path}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "db_engine = connect_to_db(DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Format the Database Schema\n",
    "\n",
    "This is a key step for Retrieval-Augmented Generation (RAG). We load the schema from `tables.json` and format it as a string to provide context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema context created successfully.\n"
     ]
    }
   ],
   "source": [
    "def get_schema_context(schema_path):\n",
    "    \"\"\"Loads the database schema from tables.json and formats it for the LLM prompt.\"\"\"\n",
    "    try:\n",
    "        with open(schema_path, 'r') as f:\n",
    "            schema_data = json.load(f)[0] # The data is inside a list\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, table_name in enumerate(schema_data['table_names_original']):\n",
    "            table_columns = [col[1] for col in schema_data['column_names_original'] if col[0] == i]\n",
    "            context_parts.append(f\"Table {table_name}, columns = [{', '.join(table_columns)}]\")\n",
    "            \n",
    "        schema_context = '\\n'.join(context_parts)\n",
    "        print(\"✅ Schema context created successfully.\")\n",
    "        # print(\"--- Schema Context Preview ---\")\n",
    "        # print(schema_context[:500] + \"...\")\n",
    "        return schema_context\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load or parse schema file: {e}\")\n",
    "        return None\n",
    "\n",
    "schema_context = get_schema_context(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the LLM (CodeS-1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading CodeS-1B model...\n",
      "📥 This may take a few minutes on the first run as it downloads the model (~2GB).\n",
      "🔥 Model loaded successfully on CPU!\n"
     ]
    }
   ],
   "source": [
    "def load_codes_model():\n",
    "    \"\"\"Load the CodeS-1B model for SQL generation.\"\"\"\n",
    "    print(\"🤖 Loading CodeS-1B model...\")\n",
    "    print(\"📥 This may take a few minutes on the first run as it downloads the model (~2GB).\")\n",
    "    \n",
    "    model_name = \"seeklhy/codes-1b\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32, # Use float32 for CPU\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"🔥 Model loaded successfully on CPU!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model, tokenizer = load_codes_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Core Functions for Text-to-SQL\n",
    "\n",
    "These functions will handle SQL generation and safe execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, schema: str, llm_model, llm_tokenizer):\n",
    "    \"\"\"Generate SQL using the loaded LLM.\"\"\"\n",
    "    if not llm_model or not llm_tokenizer:\n",
    "        return \"❌ Model not loaded\"\n",
    "\n",
    "    prompt = f\"\"\"### Instructions:\n",
    "Your task is to convert a question into a SQL query, given a database schema.\n",
    "Adhere to these rules:\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question.\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT t1.col1, t2.col2 FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id`.\n",
    "\n",
    "### Input:\n",
    "Question: {question}\n",
    "\n",
    "### Database Schema:\n",
    "{schema}\n",
    "\n",
    "### SQL Query:\"\"\"\n",
    "    \n",
    "    # Tokenize the input and create an attention mask\n",
    "    inputs = llm_tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=2048\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs, # Pass both input_ids and attention_mask\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, # Use greedy decoding for more consistent results\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the SQL query from the generated text\n",
    "    sql_start = generated_text.find(\"### SQL Query:\") + len(\"### SQL Query:\")\n",
    "    sql_query = generated_text[sql_start:].strip()\n",
    "    \n",
    "    # A simple cleanup to remove potential text after the query\n",
    "    if ';' in sql_query:\n",
    "        sql_query = sql_query.split(';')[0]\n",
    "        \n",
    "    return sql_query\n",
    "\n",
    "def execute_sql(engine, query):\n",
    "    \"\"\"Execute a SQL query and return the result as a DataFrame.\"\"\"\n",
    "    if not engine:\n",
    "        return pd.DataFrame(), \"No database connection.\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_query(text(query), conn)\n",
    "        return df, \"✅ Success\"\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), f\"❌ Query execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run a Test Query\n",
    "\n",
    "Let's test the full pipeline with a sample question from the EHRSQL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Question: How many patients are female?\n",
      "\n",
      "🤖 Generated SQL:\n",
      "```sql\n",
      "SELECT COUNT(DISTINCT gender)\n",
      "FROM patients\n",
      "JOIN admissions ON patients.subject_id = admissions.subject_id\n",
      "JOIN d_icd_diagnoses ON admissions.hadm_id = d_icd_diagnoses.hadm_id\n",
      "JOIN d_icd_procedures ON admissions.hadm_id = d_icd_procedures.hadm_id\n",
      "JOIN d_labitems ON admissions.hadm_id = d_labitems.hadm_id\n",
      "JOIN d_items ON d_labitems.itemid = d_items.itemid\n",
      "JOIN diagnoses_icd ON admissions.hadm_id = diagnoses_icd.hadm_id\n",
      "JOIN procedures_icd ON admissions.hadm_id = procedures_icd.hadm_id\n",
      "JOIN labevents ON admissions.hadm_id = labevents.hadm_id\n",
      "JOIN prescriptions ON admissions.hadm_id = prescriptions.hadm_id\n",
      "JOIN cost ON admissions.hadm_id = cost.hadm_id\n",
      "JOIN chartevents ON admissions.hadm_id = chartevents.hadm_id\n",
      "JOIN inputevents ON admissions.hadm_id = inputevents.hadm_id\n",
      "JOIN outputevents ON admissions.hadm_id = outputevents.hadm_id\n",
      "JOIN microbiologyevents ON admissions.hadm_id = microbiologyevents.hadm_id\n",
      "JOIN icustays ON admissions.hadm_id = icustays.hadm_id\n",
      "JOIN transfers ON admissions.hadm_id = transfers.hadm_id\n",
      "WHERE d_icd_diagnoses.icd_code = '400.0'\n",
      "AND d_icd_procedures.icd_code = '400.0'\n",
      "AND d_labitems.label = 'Hemoglobin'\n",
      "AND d_items.label = 'Hemoglobin'\n",
      "AND labevents.itemid = 1000000\n",
      "AND labevents.charttime BETWEEN '2018-01-01' AND '2018-01-31'\n",
      "AND chartevents.itemid = 1000000\n",
      "AND chartevents.chart\n",
      "```\n",
      "\n",
      "📊 Execution Result: ❌ Query execution error: (sqlite3.OperationalError) no such column: chartevents.chart\n",
      "[SQL: SELECT COUNT(DISTINCT gender)\n",
      "FROM patients\n",
      "JOIN admissions ON patients.subject_id = admissions.subject_id\n",
      "JOIN d_icd_diagnoses ON admissions.hadm_id = d_icd_diagnoses.hadm_id\n",
      "JOIN d_icd_procedures ON admissions.hadm_id = d_icd_procedures.hadm_id\n",
      "JOIN d_labitems ON admissions.hadm_id = d_labitems.hadm_id\n",
      "JOIN d_items ON d_labitems.itemid = d_items.itemid\n",
      "JOIN diagnoses_icd ON admissions.hadm_id = diagnoses_icd.hadm_id\n",
      "JOIN procedures_icd ON admissions.hadm_id = procedures_icd.hadm_id\n",
      "JOIN labevents ON admissions.hadm_id = labevents.hadm_id\n",
      "JOIN prescriptions ON admissions.hadm_id = prescriptions.hadm_id\n",
      "JOIN cost ON admissions.hadm_id = cost.hadm_id\n",
      "JOIN chartevents ON admissions.hadm_id = chartevents.hadm_id\n",
      "JOIN inputevents ON admissions.hadm_id = inputevents.hadm_id\n",
      "JOIN outputevents ON admissions.hadm_id = outputevents.hadm_id\n",
      "JOIN microbiologyevents ON admissions.hadm_id = microbiologyevents.hadm_id\n",
      "JOIN icustays ON admissions.hadm_id = icustays.hadm_id\n",
      "JOIN transfers ON admissions.hadm_id = transfers.hadm_id\n",
      "WHERE d_icd_diagnoses.icd_code = '400.0'\n",
      "AND d_icd_procedures.icd_code = '400.0'\n",
      "AND d_labitems.label = 'Hemoglobin'\n",
      "AND d_items.label = 'Hemoglobin'\n",
      "AND labevents.itemid = 1000000\n",
      "AND labevents.charttime BETWEEN '2018-01-01' AND '2018-01-31'\n",
      "AND chartevents.itemid = 1000000\n",
      "AND chartevents.chart]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "def run_full_test(question, schema, engine, llm_model, llm_tokenizer):\n",
    "    print(f\"❓ Question: {question}\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    generated_query = generate_sql(question, schema, llm_model, llm_tokenizer)\n",
    "    print(f\"\\n🤖 Generated SQL:\\n```sql\\n{generated_query}\\n```\")\n",
    "    \n",
    "    # Execute SQL\n",
    "    result_df, message = execute_sql(engine, generated_query)\n",
    "    \n",
    "    print(f\"\\n📊 Execution Result: {message}\")\n",
    "    if not result_df.empty:\n",
    "        display(result_df)\n",
    "\n",
    "# Load one question from the dev set for testing\n",
    "try:\n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions_data = json.load(f)['data']\n",
    "    \n",
    "    # Let's pick a specific, interesting question for a clear test\n",
    "    test_question = \"How many patients are female?\"\n",
    "    \n",
    "    # Run the test if all components are ready\n",
    "    if db_engine and schema_context and model and tokenizer:\n",
    "        run_full_test(test_question, schema_context, db_engine, model, tokenizer)\n",
    "    else:\n",
    "        print(\"⚠️ Cannot run test. One or more components (DB, Schema, Model) failed to load.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not load or run test questions: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
