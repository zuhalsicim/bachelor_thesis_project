{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Update the `PROJECT_PATH` variable to the absolute path of your `ehrsql-2024` project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n",
      "Schema Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/tables.json\n",
      "Questions Path: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/test/data.json\n"
     ]
    }
   ],
   "source": [
    "PROJECT_PATH = 'C:/Uni/Bachelorarbeit/ehrsql-2024'\n",
    "\n",
    "DB_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/mimic_iv.sqlite')\n",
    "SCHEMA_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/tables.json')\n",
    "QUESTIONS_PATH = os.path.join(PROJECT_PATH, 'data/mimic_iv/test/data.json')\n",
    "\n",
    "\n",
    "print(f\"Database Path: {DB_PATH}\")\n",
    "print(f\"Schema Path: {SCHEMA_PATH}\")\n",
    "print(f\"Questions Path: {QUESTIONS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"‚ùå ERROR: Database file not found. Please check your PROJECT_PATH.\")\n",
    "if not os.path.exists(SCHEMA_PATH):\n",
    "    print(\"‚ùå ERROR: Schema file not found. Please check your PROJECT_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful to: C:/Uni/Bachelorarbeit/ehrsql-2024\\data/mimic_iv/mimic_iv.sqlite\n"
     ]
    }
   ],
   "source": [
    "def connect_to_db(db_path):\n",
    "    \"\"\"Create a connection engine for the SQLite database.\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'sqlite:///{db_path}')\n",
    "        with engine.connect() as conn:\n",
    "            print(f\"‚úÖ Database connection successful to: {db_path}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "db_engine = connect_to_db(DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Format the Database Schema\n",
    "\n",
    "This is a key step for Retrieval-Augmented Generation (RAG). We load the schema from `tables.json` and format it as a string to provide context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema context created successfully.\n"
     ]
    }
   ],
   "source": [
    "def get_schema_context(schema_path):\n",
    "    \"\"\"Loads the database schema from tables.json and formats it for the LLM prompt.\"\"\"\n",
    "    try:\n",
    "        with open(schema_path, 'r') as f:\n",
    "            schema_data = json.load(f)[0] # The data is inside a list\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, table_name in enumerate(schema_data['table_names_original']):\n",
    "            table_columns = [col[1] for col in schema_data['column_names_original'] if col[0] == i]\n",
    "            context_parts.append(f\"Table {table_name}, columns = [{', '.join(table_columns)}]\")\n",
    "            \n",
    "        schema_context = '\\n'.join(context_parts)\n",
    "        print(\"‚úÖ Schema context created successfully.\")\n",
    "        return schema_context\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load or parse schema file: {e}\")\n",
    "        return None\n",
    "\n",
    "schema_context = get_schema_context(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the LLM (CodeS-7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading CodeS-7B model...\n",
      "üì• This may take a few minutes on the first run as it downloads the model.\n",
      "‚ùå Failed to load model: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\n"
     ]
    }
   ],
   "source": [
    "def load_codes_model():\n",
    "    \"\"\"Load the CodeS-7B model for SQL generation.\"\"\"\n",
    "    print(\"ü§ñ Loading CodeS-7B model...\")\n",
    "    print(\"üì• This may take a few minutes on the first run as it downloads the model.\")\n",
    "    \n",
    "    model_name = \"seeklhy/codes-7b\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üî• Model loaded successfully on {device}!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model, tokenizer = load_codes_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Core Functions for Text-to-SQL\n",
    "\n",
    "These functions will handle SQL generation and safe execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, schema: str, llm_model, llm_tokenizer):\n",
    "    \"\"\"Generate SQL using the loaded LLM.\"\"\"\n",
    "    if not llm_model or not llm_tokenizer:\n",
    "        return \"‚ùå Model not loaded\"\n",
    "\n",
    "    prompt = f\"\"\"### Instructions:\\n\n",
    "Your task is to convert a question into a SQL query, given a database schema.\\n\n",
    "Adhere to these rules:\\n\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question.\\n\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT t1.col1, t2.col2 FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id`.\\n\n",
    "\\n\n",
    "### Input:\\n\n",
    "Question: {question}\\n\n",
    "\\n\n",
    "### Database Schema:\\n\n",
    "{schema}\\n\n",
    "\\n\n",
    "### SQL Query:\"\"\"\n",
    "    \n",
    "    inputs = llm_tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, # Use greedy decoding for more consistent results\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the SQL query from the generated text\n",
    "    sql_start = generated_text.find(\"### SQL Query:\") + len(\"### SQL Query:\")\n",
    "    sql_query = generated_text[sql_start:].strip()\n",
    "    \n",
    "    # A simple cleanup to remove potential text after the query\n",
    "    if ';' in sql_query:\n",
    "        sql_query = sql_query.split(';')[0]\n",
    "        \n",
    "    return sql_query\n",
    "\n",
    "def execute_sql(engine, query):\n",
    "    \"\"\"Execute a SQL query and return the result as a DataFrame.\"\"\"\n",
    "    if not engine:\n",
    "        return pd.DataFrame(), \"No database connection.\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql_query(text(query), conn)\n",
    "        return df, \"‚úÖ Success\"\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), f\"‚ùå Query execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run a Test Query\n",
    "\n",
    "Let's test the full pipeline with a sample question from the EHRSQL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Cannot run test. One or more components (DB, Schema, Model) failed to load.\n"
     ]
    }
   ],
   "source": [
    "def run_full_test(question, schema, engine, llm_model, llm_tokenizer):\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    generated_query = generate_sql(question, schema, llm_model, llm_tokenizer)\n",
    "    print(f\"\\nü§ñ Generated SQL:\\n```sql\\n{generated_query}\\n```\")\n",
    "    \n",
    "    # Execute SQL\n",
    "    result_df, message = execute_sql(engine, generated_query)\n",
    "    \n",
    "    print(f\"\\nüìä Execution Result: {message}\")\n",
    "    if not result_df.empty:\n",
    "        display(result_df)\n",
    "\n",
    "# Load one question from the test set for testing\n",
    "try:\n",
    "    with open(QUESTIONS_PATH, 'r') as f:\n",
    "        questions_data = json.load(f)['data']\n",
    "    \n",
    "    test_question = questions_data[0]['question']\n",
    "    \n",
    "    # Run the test if all components are ready\n",
    "    if db_engine and schema_context and model and tokenizer:\n",
    "        run_full_test(test_question, schema_context, db_engine, model, tokenizer)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cannot run test. One or more components (DB, Schema, Model) failed to load.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load or run test questions: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
